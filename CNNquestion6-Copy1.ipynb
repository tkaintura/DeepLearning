{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "944f4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f41d0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(Conv2D, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self.conv = nn.Parameter(torch.ones(out_channels, in_channels, kernel_size, kernel_size))\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        b, c, h, w = input_batch.size()\n",
    "        k = self.kernel_size\n",
    "        p = self.padding\n",
    "        s = self.stride\n",
    "        \n",
    "        h_out = (h + 2*p - k)/s + 1\n",
    "        w_out = (w + 2*p - k)/s + 1\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "        #Unfold\n",
    "        x = torch.nn.functional.unfold(input_batch, (k, k), padding=p)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        P = x.shape[1]\n",
    "        \n",
    "        # Reshape to (b*p, k)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2]))\n",
    "        \n",
    "        #Matmul\n",
    "        W = torch.reshape(self.conv , (self.conv.size(0), -1))\n",
    "        y = x.matmul(W.t())\n",
    "        \n",
    "        # Reshape to (b, l, p)\n",
    "        y = torch.reshape(y, (input_batch.shape[0], P, y.shape[1]))\n",
    "        \n",
    "        y = y.transpose(1, 2)\n",
    "        \n",
    "        out = torch.nn.functional.fold(y, (h_out, w_out), (1, 1))\n",
    "        print(out.shape)\n",
    "        \n",
    "        r = self.conv1(input_batch)\n",
    "        assert r.shape == out.shape\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33556aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "conv = Conv2D(in_channels = 3, out_channels = 10)\n",
    "input_batch = torch.randn(16, 3, 32, 32)\n",
    "output_batch = conv(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd43a5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        \n",
    "        # store objects for the backward\n",
    "        ctx.save_for_backward(input_batch)\n",
    "        ctx.save_for_backward(kernel)\n",
    "        \n",
    "        h = input_batch.shape[2]\n",
    "        w = input_batch.shape[3]\n",
    "        c = input_batch.shape[1]\n",
    "        k = kernel.shape[2]\n",
    "        C = kernel.shape[0]\n",
    "\n",
    "            \n",
    "        #output_batch = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        h_out = (h + 2*padding - k)/stride + 1\n",
    "        w_out = (w + 2*padding - k)/stride + 1\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "        \n",
    "\n",
    "        #Unfold\n",
    "        x = torch.nn.functional.unfold(input_batch, (k, k), padding=padding)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        P = x.shape[1]\n",
    "        \n",
    "        # Reshape to (b*p, k)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2]))\n",
    "        ctx.save_for_backward(x)\n",
    "        \n",
    "        #Matmul\n",
    "        W = torch.reshape(kernel , (kernel.size(0), -1))\n",
    "        #ctx.save_for_backward(W)\n",
    "        y = x.matmul(W.t())\n",
    "        \n",
    "        \n",
    "        # Reshape to (b, l, p)\n",
    "        y = torch.reshape(y, (input_batch.shape[0], P, y.shape[1]))\n",
    "        yshape = y.shape\n",
    "        ctx.yshape = yshape\n",
    "        y = y.transpose(1, 2)\n",
    "    \n",
    "        out = torch.nn.functional.fold(y, (h_out, w_out), (1, 1))\n",
    "        print(out.shape)\n",
    "        \n",
    "        r = F.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "        assert r.shape == out.shape\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \n",
    "        # retrieve stored objects\n",
    "        input, kernel, U = ctx.saved_tensors\n",
    "        \n",
    "        \n",
    "        # backward of Kernel\n",
    "        yshape = ctx.yshape\n",
    "        \n",
    "        y_grad = torch.reshape(grad_output, yshape)\n",
    "        y_grad = torch.reshape(yshape, (yhape[0] * yshape[1], yshape[2]))\n",
    "        \n",
    "        kernel_grad = U.t().matmul(y_grad)\n",
    "        \n",
    "        input_batch_grad = None\n",
    "        \n",
    "        return input_batch_grad, kernel_grad, None, None\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f10fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 10, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "input_batch = torch.randn(16, 3, 32, 32,  requires_grad=True, dtype=torch.double)\n",
    "kernel = torch.randn(10, 3, 3, 3,  requires_grad=True, dtype=torch.double)\n",
    "out = Conv2DFunc.apply(input_batch, kernel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
